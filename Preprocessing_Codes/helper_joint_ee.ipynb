{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "#import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#from nltk.tokenize import SpaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def BERTPOS(sent_file, bert_pos_file):<br>\n",
    "#     sents = open(sent_file).readlines()<br>\n",
    "#     bert_model_name = 'bert-base-uncased'<br>\n",
    "#     #bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True)<br>\n",
    "#     bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\", do_basic_tokenize=False)<br>\n",
    "#     writer = open(bert_pos_file, 'w')<br>\n",
    "#     for sent in sents:<br>\n",
    "#         sent = sent.strip()<br>\n",
    "#         seq = list()<br>\n",
    "#         tk = SpaceTokenizer()<br>\n",
    "#         text = tk.tokenize(sent)<br>\n",
    "#         tags = nltk.pos_tag(text)<br>\n",
    "#         for t in tags:<br>\n",
    "#             t1 = t[1]<br>\n",
    "#             seq.append(t1)<br>\n",
    "#         bert_seq = []<br>\n",
    "#         tokens = sent.split(' ')<br>\n",
    "#         for j in range(len(tokens)):<br>\n",
    "#             sub_tokens = bert_tokenizer.wordpiece_tokenizer.tokenize(tokens[j].lower())<br>\n",
    "#             if len(sub_tokens) == 0:<br>\n",
    "#                 sub_tokens = [tokens[j]]<br>\n",
    "#             for k in range(len(sub_tokens)):<br>\n",
    "#                 bert_seq.append(seq[j])<br>\n",
    "#         writer.write(' '.join(bert_seq) + '\\n')<br>\n",
    "#     writer.close()<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BERTData(sent_file, pointer_file, bert_sent_file, bert_pointer_file):#only changed here\n",
    "    bert_model_name = 'bert-base-cased'\n",
    "    sents = open(sent_file).readlines()\n",
    "    #pos_lists = open(pos_file).readlines()\n",
    "    #ent_lists = open(ent_file).readlines()#\n",
    "    pointer_lines = open(pointer_file).readlines()\n",
    "    #bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True)\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\", do_basic_tokenize=False)\n",
    "    writer1 = open(bert_sent_file, 'w+')\n",
    "    writer2 = open(bert_pointer_file, 'w+')\n",
    "    #writer3 = open(bert_pos_file, 'w+')\n",
    "    #writer4 = open(bert_ent_file, 'w+')\n",
    "    max_len = 0\n",
    "    max_len_tok = 0\n",
    "    max_trg_len=0\n",
    "    #max_len_et=0\n",
    "    for i in tqdm(range(len(sents))):\n",
    "        sent = sents[i].strip()#sentence\n",
    "        #pos_seq = pos_lists[i].strip().split()#pos tag list\n",
    "        #ent_seq = ent_lists[i].strip().split()#ent tag\n",
    "        #print(pos_seq)\n",
    "        #pos_seq = list() a bcd. sjg\n",
    "        #tk = SpaceTokenizer()\n",
    "        #tags = nltk.pos_tag(tk.tokenize(sent))\n",
    "        #for t in tags:\n",
    "            #t1 = t[1]\n",
    "            #pos_seq.append(t1)#all the corresponding pos tags\n",
    "        tokens = sent.split(' ')#token list\n",
    "        bert_tokens = []###\n",
    "        token_map = OrderedDict()\n",
    "        bert_idx = 0\n",
    "        #bert_pos_seq = []###\n",
    "        #bert_ent_seq = []###\n",
    "        for j in range(len(tokens)):\n",
    "            sub_tokens = bert_tokenizer.tokenize(tokens[j])\n",
    "            if len(sub_tokens) == 0:\n",
    "                sub_tokens = [tokens[j]]\n",
    "            bert_tokens += sub_tokens\n",
    "            token_map[j] = (bert_idx, bert_idx + len(sub_tokens) - 1)\n",
    "            bert_idx += len(sub_tokens)\n",
    "            #bert_pos_seq += [pos_seq[j] for k in range(len(sub_tokens))]\n",
    "            #bert_ent_seq += [ent_seq[j] for l in range(len(sub_tokens))]\n",
    "\n",
    "        #print(bert_pos_seq)\n",
    "        #assert len(bert_tokens) == len(bert_pos_seq)\n",
    "        #if max_len < len(bert_pos_seq):\n",
    "        #    max_len = len(bert_pos_seq)\n",
    "        if max_len_tok < len(bert_tokens):\n",
    "            max_len_tok = len(bert_tokens)\n",
    "\n",
    "        #if max_len_et < len(bert_ent_seq):\n",
    "        #    max_len_et = len(bert_ent_seq)\n",
    "        bert_pointers = []\n",
    "        pointer_line = pointer_lines[i].strip()\n",
    "        pointers = pointer_line.split(' | ')\n",
    "        for p in pointers:\n",
    "            ev, t_s, t_e, a_s, a_e= p.split()#trigger_start, trigger_end, event_type, company_start, company_end\n",
    "            new_p = [str(token_map[int(t_s)][0]), str(token_map[int(t_e)][1]), ev, str(token_map[int(a_s)][0]), str(token_map[int(a_e)][1])]\n",
    "            bert_pointers.append(' '.join(new_p))\n",
    "        #print(bert_pointers)\n",
    "        if max_trg_len < len(bert_pointers):\n",
    "            max_trg_len = len(bert_pointers)\n",
    "        bert_sent = ' '.join(bert_tokens)\n",
    "        #bert_pos = ' '.join(bert_pos_seq)\n",
    "        #bert_ent = ' '.join(bert_ent_seq)\n",
    "        bert_pointer_line = ' | '.join(bert_pointers)\n",
    "        # print(sent)\n",
    "        # print(pointer_line)\n",
    "        # print(bert_sent)\n",
    "        # print(bert_pointer_line)\n",
    "        # print('\\n\\n')\n",
    "        writer1.write(bert_sent + '\\n')\n",
    "        writer2.write(bert_pointer_line + '\\n')\n",
    "        #writer3.write(bert_pos + '\\n')\n",
    "        #writer4.write(bert_ent + '\\n')\n",
    "    writer1.close()\n",
    "    writer2.close()\n",
    "    #writer3.close()\n",
    "    #writer4.close()\n",
    "    #return max_len_tok, max_len, max_trg_len, max_len_et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DepDist():\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2193/2193 [00:00<00:00, 2490.74it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sent_file='sent5.txt'#print(sys.argv[1]) 'dev_oct.sent'\n",
    "    point_file='pointer5.txt'#print(sys.argv[2])'dev_trim_oct.pointer'\n",
    "    #pos_file='dev_oct.pos'#print(sys.argv[3])\n",
    "    #ent_file='dev_oct.ent'\n",
    "    bert_sent_file='sent5.sent'#print(sys.argv[4])\n",
    "    bert_point_file='pointer5.pointer'#print(sys.argv[5])\n",
    "    #bert_pos_file='dev_bert.pos'\n",
    "    #bert_ent_file='dev_bert.ent'\n",
    "    #BERTData(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4], sys.argv[5])\n",
    "    #max1, max2, max3, max4 = BERTData(sent_file, point_file, bert_sent_file, bert_point_file)\n",
    "    BERTData(sent_file, point_file, bert_sent_file, bert_point_file)\n",
    "    #print('{},{},{},{}'.format(max1, max2, max3, max4))\n",
    "    # BERTPOS(sys.argv[1], sys.argv[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
